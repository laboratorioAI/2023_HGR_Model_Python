{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Function model\n",
    "def set_neural_network_architecture(input_size, num_classes):\n",
    "\n",
    "    # Entradas\n",
    "    sequence=tf.keras.layers.Input(shape=input_size)\n",
    "    \n",
    "    # Add layer branches\n",
    "    # Primer bloque\n",
    "    Inception_1a_3x3_reduce = tf.keras.layers.Conv2D(16, (1, 1))(sequence)\n",
    "    Inception_1a_3x3_relu_reduce = tf.keras.layers.ReLU()(Inception_1a_3x3_reduce)\n",
    "    Inception_1a_3x3 = tf.keras.layers.Conv2D(18, (3, 3), padding='same')(Inception_1a_3x3_relu_reduce)\n",
    "    \n",
    "    Inception_1a_1x1 = tf.keras.layers.Conv2D(18, (1, 1))(sequence)\n",
    "    \n",
    "    Inception_1a_pool = tf.keras.layers.MaxPooling2D((3, 3), padding='same', strides=(1,1))(sequence)\n",
    "    Inception_1a_pool_proj = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1a_pool)\n",
    "\n",
    "    \n",
    "    Inception_1a_5x5_reduce = tf.keras.layers.Conv2D(16, (1, 1))(sequence)\n",
    "    Inception_1a_5x5_relu_reduce = tf.keras.layers.ReLU()(Inception_1a_5x5_reduce)\n",
    "    Inception_1a_5x5 = tf.keras.layers.Conv2D(18, (5, 5), padding='same')(Inception_1a_5x5_relu_reduce)\n",
    "\n",
    "    depthcat_1a = tf.keras.layers.Concatenate(axis=-1)([Inception_1a_3x3, Inception_1a_1x1, Inception_1a_pool_proj, Inception_1a_5x5])\n",
    "    Inception_1a_relu = tf.keras.layers.ReLU()(depthcat_1a)\n",
    "\n",
    "    # Segundo bloque\n",
    "    Inception_1b_3x3_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Inception_1a_relu)\n",
    "    Inception_1b_3x3_relu_reduce = tf.keras.layers.ReLU()(Inception_1b_3x3_reduce)\n",
    "    Inception_1b_3x3 = tf.keras.layers.Conv2D(18, (3, 3), padding='same')(Inception_1b_3x3_relu_reduce)\n",
    "    \n",
    "    Inception_1b_1x1 = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1a_relu)\n",
    "    \n",
    "    Inception_1b_pool = tf.keras.layers.MaxPooling2D((3, 3), padding='same', strides=(1,1))(Inception_1a_relu)\n",
    "    Inception_1b_pool_proj = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1b_pool)\n",
    "\n",
    "    \n",
    "    Inception_1b_5x5_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Inception_1a_relu)\n",
    "    Inception_1b_5x5_relu_reduce = tf.keras.layers.ReLU()(Inception_1b_5x5_reduce)\n",
    "    Inception_1b_5x5 = tf.keras.layers.Conv2D(18, (5, 5), padding='same')(Inception_1b_5x5_relu_reduce)\n",
    "\n",
    "    depthcat_1b = tf.keras.layers.Concatenate(axis=-1)([Inception_1b_3x3,Inception_1b_1x1,Inception_1b_pool_proj,Inception_1b_5x5])\n",
    "    Inception_1b_relu = tf.keras.layers.ReLU()(depthcat_1b)\n",
    "    \n",
    "\n",
    "    # Tercer bloque\n",
    "    Inception_1c_3x3_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Inception_1b_relu)\n",
    "    Inception_1c_3x3_relu_reduce = tf.keras.layers.ReLU()(Inception_1c_3x3_reduce)\n",
    "    Inception_1c_3x3 = tf.keras.layers.Conv2D(18, (3, 3), padding='same')(Inception_1c_3x3_relu_reduce)\n",
    "    \n",
    "    Inception_1c_1x1 = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1b_relu)\n",
    "    \n",
    "    Inception_1c_pool = tf.keras.layers.MaxPooling2D((3, 3), padding='same', strides=(1,1))(Inception_1b_relu)\n",
    "    Inception_1c_pool_proj = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1c_pool)\n",
    "\n",
    "    \n",
    "    Inception_1c_5x5_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Inception_1b_relu)\n",
    "    Inception_1c_5x5_relu_reduce = tf.keras.layers.ReLU()(Inception_1c_5x5_reduce)\n",
    "    Inception_1c_5x5 = tf.keras.layers.Conv2D(18, (5, 5), padding='same')(Inception_1c_5x5_relu_reduce)\n",
    "\n",
    "    depthcat_1c = tf.keras.layers.Concatenate(axis=-1)([Inception_1c_3x3,Inception_1c_1x1,Inception_1c_pool_proj,Inception_1c_5x5])\n",
    "    \n",
    "    #       Adición Layer\n",
    "\n",
    "    Addition_1 = tf.keras.layers.Add()([depthcat_1c, Inception_1a_relu])\n",
    "    Addition_1_relu = tf.keras.layers.ReLU()(Addition_1)\n",
    "\n",
    "    # Cuarto bloque\n",
    "    \n",
    "    Inception_1d_3x3_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Addition_1_relu)\n",
    "    Inception_1d_3x3_relu_reduce = tf.keras.layers.ReLU()(Inception_1d_3x3_reduce)\n",
    "    Inception_1d_3x3 = tf.keras.layers.Conv2D(18, (3, 3), padding='same')(Inception_1d_3x3_relu_reduce)\n",
    "    \n",
    "    Inception_1d_1x1 = tf.keras.layers.Conv2D(18, (1, 1))(Addition_1_relu)\n",
    "    \n",
    "    Inception_1d_pool = tf.keras.layers.MaxPooling2D((3, 3), padding='same', strides=(1,1))(Addition_1_relu)\n",
    "    Inception_1d_pool_proj = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1d_pool)\n",
    "\n",
    "    \n",
    "    Inception_1d_5x5_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Addition_1_relu)\n",
    "    Inception_1d_5x5_relu_reduce = tf.keras.layers.ReLU()(Inception_1d_5x5_reduce)\n",
    "    Inception_1d_5x5 = tf.keras.layers.Conv2D(18, (5, 5), padding='same')(Inception_1d_5x5_relu_reduce)\n",
    "\n",
    "    depthcat_1d = tf.keras.layers.Concatenate(axis=-1)([Inception_1d_3x3,Inception_1d_1x1,Inception_1d_pool_proj,Inception_1d_5x5])\n",
    "    Inception_1d_relu = tf.keras.layers.ReLU()(depthcat_1d)\n",
    "\n",
    "    # Quinto bloque\n",
    "    \n",
    "    Inception_1e_3x3_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Inception_1d_relu)\n",
    "    Inception_1e_3x3_relu_reduce = tf.keras.layers.ReLU()(Inception_1e_3x3_reduce)\n",
    "    Inception_1e_3x3 = tf.keras.layers.Conv2D(18, (3, 3), padding='same')(Inception_1e_3x3_relu_reduce)\n",
    "    \n",
    "    Inception_1e_1x1 = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1d_relu)\n",
    "    \n",
    "    Inception_1e_pool = tf.keras.layers.MaxPooling2D((3, 3), padding='same', strides=(1,1))(Inception_1d_relu)\n",
    "    Inception_1e_pool_proj = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1e_pool)\n",
    "\n",
    "    \n",
    "    Inception_1e_5x5_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Inception_1d_relu)\n",
    "    Inception_1e_5x5_relu_reduce = tf.keras.layers.ReLU()(Inception_1e_5x5_reduce)\n",
    "    Inception_1e_5x5 = tf.keras.layers.Conv2D(18, (5, 5), padding='same')(Inception_1e_5x5_relu_reduce)\n",
    "\n",
    "    depthcat_1e = tf.keras.layers.Concatenate(axis=-1)([Inception_1e_3x3,Inception_1e_1x1,Inception_1e_pool_proj,Inception_1e_5x5])\n",
    "    \n",
    "    #       Adición Layer\n",
    "\n",
    "    Addition_2 = tf.keras.layers.Add()([depthcat_1e, Addition_1_relu])\n",
    "    Addition_2_relu = tf.keras.layers.ReLU()(Addition_2)\n",
    "\n",
    "    # Sexto bloque\n",
    "    \n",
    "    Inception_1f_3x3_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Addition_2_relu)\n",
    "    Inception_1f_3x3_relu_reduce = tf.keras.layers.ReLU()(Inception_1f_3x3_reduce)\n",
    "    Inception_1f_3x3 = tf.keras.layers.Conv2D(18, (3, 3), padding='same')(Inception_1f_3x3_relu_reduce)\n",
    "    \n",
    "    Inception_1f_1x1 = tf.keras.layers.Conv2D(18, (1, 1))(Addition_2_relu)\n",
    "    \n",
    "    Inception_1f_pool = tf.keras.layers.MaxPooling2D((3, 3), padding='same', strides=(1,1))(Addition_2_relu)\n",
    "    Inception_1f_pool_proj = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1f_pool)\n",
    "\n",
    "    \n",
    "    Inception_1f_5x5_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Addition_2_relu)\n",
    "    Inception_1f_5x5_relu_reduce = tf.keras.layers.ReLU()(Inception_1f_5x5_reduce)\n",
    "    Inception_1f_5x5 = tf.keras.layers.Conv2D(18, (5, 5), padding='same')(Inception_1f_5x5_relu_reduce)\n",
    "\n",
    "    depthcat_1f = tf.keras.layers.Concatenate(axis=-1)([Inception_1f_3x3,Inception_1f_1x1,Inception_1f_pool_proj,Inception_1f_5x5])\n",
    "\n",
    "    # Normalizacion\n",
    "    Batch_normalization = tf.keras.layers.BatchNormalization()(depthcat_1f)\n",
    "\n",
    "    # Aplanado\n",
    "    Flatten = tf.keras.layers.Flatten()(Batch_normalization)\n",
    "\n",
    "    reshape = tf.keras.layers.Reshape((-1, 1))(Flatten)\n",
    "\n",
    "    # LSTM\n",
    "    Lstm =  tf.keras.layers.LSTM(128, return_sequences=True)(reshape)\n",
    "\n",
    "    dense_classification = tf.keras.layers.Dense(num_classes, activation='softmax', name='classoutput')(Lstm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=sequence, outputs=dense_classification)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de datos\n",
    "\n",
    "# Directorio que contiene los archivos JSON\n",
    "directorio = 'StaticData'\n",
    "\n",
    "# Patrón para buscar archivos JSON\n",
    "patron_archivos = os.path.join(directorio, '*.json')\n",
    "\n",
    "# Obtener la lista de archivos JSON en el directorio\n",
    "archivos_json = glob.glob(patron_archivos)\n",
    "\n",
    "# Lista para almacenar los DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterar sobre los archivos JSON\n",
    "for archivo_json in archivos_json:\n",
    "    # Cargar el archivo JSON como DataFrame\n",
    "    df = pd.read_json(archivo_json)\n",
    "    # Agregar el DataFrame a la lista\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combinar los DataFrames en uno solo\n",
    "df_completo = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filtrar los valores nulos en la columna \"Spectograms\"\n",
    "df_completo_filtrado = df_completo.dropna(subset=['Spectograms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Spectograms    Gesture  \\\n",
      "0        [[[0.0, 1e-10, 6.06e-08, 3.6e-09, 1e-10, 1.37e...  noGesture   \n",
      "1        [[[0.0, 3e-10, 2.0950000000000002e-07, 4.5e-09...  noGesture   \n",
      "2        [[[0.0, 1.2e-09, 2.079e-07, 3.3e-09, 3.3e-09, ...  noGesture   \n",
      "3        [[[2e-10, 1.9e-09, 8.05e-08, 4e-10, 2e-09, 1.7...     waveIn   \n",
      "4        [[[2e-10, 1e-09, 6.3e-09, 5e-10, 0.0, 7.6e-09,...     waveIn   \n",
      "...                                                    ...        ...   \n",
      "6221706  [[[3.717e-07, 1.8100000000000002e-07, 2.99e-08...     waveIn   \n",
      "6221707  [[[1.842e-07, 1.479e-07, 7.45e-08, 1.554e-07, ...     waveIn   \n",
      "6221708  [[[5.21e-08, 5.21e-08, 1.0230000000000001e-07,...  noGesture   \n",
      "6221709  [[[2.1e-09, 3.4000000000000003e-09, 1.312e-07,...  noGesture   \n",
      "6221710  [[[5.7000000000000006e-09, 3.3e-09, 1.133e-07,...  noGesture   \n",
      "\n",
      "         Timestamp                                  Quat_Spectrograms  0  \n",
      "0            240.0  [[[0.0, 8.023140852e-37, 0.0, 0.0], [4.1998258...  0  \n",
      "1            255.0  [[[7.224118205e-36, 2.006177025e-37, 2.0543252...  0  \n",
      "2            270.0  [[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], ...  0  \n",
      "3            285.0  [[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], ...  0  \n",
      "4            300.0  [[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], ...  0  \n",
      "...            ...                                                ... ..  \n",
      "6221706      630.0  [[[0.0, 0.0, 0.0, 3.922673628e-36], [0.0, 0.0,...  0  \n",
      "6221707      645.0  [[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 3e-10, 2e-1...  0  \n",
      "6221708      660.0  [[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], ...  0  \n",
      "6221709      675.0  [[[2.7813611829999997e-36, 0.0, 0.0, 0.0], [0....  0  \n",
      "6221710      690.0  [[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], ...  0  \n",
      "\n",
      "[106579 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Muestra de los datos\n",
    "print(df_completo_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide los datos en entrenamiento y validación\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Obtén las características y las etiquetas de entrenamiento y validación\n",
    "x_train = train_df['Spectograms'].values\n",
    "y_train = train_df['Gesture'].values\n",
    "x_val = val_df['Spectograms'].values\n",
    "y_val = val_df['Gesture'].values\n",
    "\n",
    "# Convierte las características y las etiquetas en matrices NumPy\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\USER\\Documents\\Tesis\\CodigoTesis\\pruebas.ipynb Cell 5\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/Tesis/CodigoTesis/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39mvalidation_patience)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/Tesis/CodigoTesis/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Entrenamiento del modelo\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/Tesis/CodigoTesis/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(x_train, y_train, epochs\u001b[39m=\u001b[39mmax_epochs, batch_size\u001b[39m=\u001b[39mmini_batch_size, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/Tesis/CodigoTesis/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                     validation_data\u001b[39m=\u001b[39m(x_val, y_val), shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/Tesis/CodigoTesis/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                     callbacks\u001b[39m=\u001b[39m[lr_scheduler, early_stopping])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Configuración de las opciones de entrenamiento\n",
    "gpu_device = \"/gpu:1\"\n",
    "max_epochs = 60\n",
    "mini_batch_size = 64\n",
    "initial_learn_rate = 0.04\n",
    "learn_rate_drop_factor = 0.2\n",
    "learn_rate_drop_period = 8\n",
    "gradient_threshold = 1\n",
    "validation_patience = 5\n",
    "\n",
    "input_size = (13,24,8)\n",
    "num_classes = 6\n",
    "\n",
    "# Configuración del entorno GPU\n",
    "with tf.device(gpu_device):\n",
    "    # Definir la arquitectura de la red neuronal\n",
    "    model = set_neural_network_architecture(input_size, num_classes)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer=Adam(learning_rate=initial_learn_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Configuración de las callbacks\n",
    "lr_scheduler = LearningRateScheduler(lambda epoch, lr: lr * learn_rate_drop_factor if epoch % learn_rate_drop_period == 0 else lr)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=validation_patience)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(x_train, y_train, epochs=max_epochs, batch_size=mini_batch_size, \n",
    "                    validation_data=(x_val, y_val), shuffle=False, verbose=1,\n",
    "                    callbacks=[lr_scheduler, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[['loss', 'val_loss']].plot(figsize=(10, 6))\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[['accuracy', 'val_accuracy']].plot(figsize=(10, 6))\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curso_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
