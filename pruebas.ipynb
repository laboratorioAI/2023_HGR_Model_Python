{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Function model\n",
    "def set_neural_network_architecture(input_size, num_classes):\n",
    "\n",
    "    # Entradas\n",
    "    sequence=tf.keras.layers.Input(shape=input_size)\n",
    "    \n",
    "    # Add layer branches\n",
    "    # Primer bloque\n",
    "    Inception_1a_3x3_reduce = tf.keras.layers.Conv2D(16, (1, 1))(sequence)\n",
    "    Inception_1a_3x3_relu_reduce = tf.keras.layers.ReLU()(Inception_1a_3x3_reduce)\n",
    "    Inception_1a_3x3 = tf.keras.layers.Conv2D(18, (3, 3), padding='same')(Inception_1a_3x3_relu_reduce)\n",
    "    \n",
    "    Inception_1a_1x1 = tf.keras.layers.Conv2D(18, (1, 1))(sequence)\n",
    "    \n",
    "    Inception_1a_pool = tf.keras.layers.MaxPooling2D((3, 3), padding='same', strides=(1,1))(sequence)\n",
    "    Inception_1a_pool_proj = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1a_pool)\n",
    "\n",
    "    \n",
    "    Inception_1a_5x5_reduce = tf.keras.layers.Conv2D(16, (1, 1))(sequence)\n",
    "    Inception_1a_5x5_relu_reduce = tf.keras.layers.ReLU()(Inception_1a_5x5_reduce)\n",
    "    Inception_1a_5x5 = tf.keras.layers.Conv2D(18, (5, 5), padding='same')(Inception_1a_5x5_relu_reduce)\n",
    "\n",
    "    depthcat_1a = tf.keras.layers.Concatenate(axis=-1)([Inception_1a_3x3, Inception_1a_1x1, Inception_1a_pool_proj, Inception_1a_5x5])\n",
    "    Inception_1a_relu = tf.keras.layers.ReLU()(depthcat_1a)\n",
    "\n",
    "    # Segundo bloque\n",
    "    Inception_1b_3x3_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Inception_1a_relu)\n",
    "    Inception_1b_3x3_relu_reduce = tf.keras.layers.ReLU()(Inception_1b_3x3_reduce)\n",
    "    Inception_1b_3x3 = tf.keras.layers.Conv2D(18, (3, 3), padding='same')(Inception_1b_3x3_relu_reduce)\n",
    "    \n",
    "    Inception_1b_1x1 = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1a_relu)\n",
    "    \n",
    "    Inception_1b_pool = tf.keras.layers.MaxPooling2D((3, 3), padding='same', strides=(1,1))(Inception_1a_relu)\n",
    "    Inception_1b_pool_proj = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1b_pool)\n",
    "\n",
    "    \n",
    "    Inception_1b_5x5_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Inception_1a_relu)\n",
    "    Inception_1b_5x5_relu_reduce = tf.keras.layers.ReLU()(Inception_1b_5x5_reduce)\n",
    "    Inception_1b_5x5 = tf.keras.layers.Conv2D(18, (5, 5), padding='same')(Inception_1b_5x5_relu_reduce)\n",
    "\n",
    "    depthcat_1b = tf.keras.layers.Concatenate(axis=-1)([Inception_1b_3x3,Inception_1b_1x1,Inception_1b_pool_proj,Inception_1b_5x5])\n",
    "    Inception_1b_relu = tf.keras.layers.ReLU()(depthcat_1b)\n",
    "    \n",
    "\n",
    "    # Tercer bloque\n",
    "    Inception_1c_3x3_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Inception_1b_relu)\n",
    "    Inception_1c_3x3_relu_reduce = tf.keras.layers.ReLU()(Inception_1c_3x3_reduce)\n",
    "    Inception_1c_3x3 = tf.keras.layers.Conv2D(18, (3, 3), padding='same')(Inception_1c_3x3_relu_reduce)\n",
    "    \n",
    "    Inception_1c_1x1 = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1b_relu)\n",
    "    \n",
    "    Inception_1c_pool = tf.keras.layers.MaxPooling2D((3, 3), padding='same', strides=(1,1))(Inception_1b_relu)\n",
    "    Inception_1c_pool_proj = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1c_pool)\n",
    "\n",
    "    \n",
    "    Inception_1c_5x5_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Inception_1b_relu)\n",
    "    Inception_1c_5x5_relu_reduce = tf.keras.layers.ReLU()(Inception_1c_5x5_reduce)\n",
    "    Inception_1c_5x5 = tf.keras.layers.Conv2D(18, (5, 5), padding='same')(Inception_1c_5x5_relu_reduce)\n",
    "\n",
    "    depthcat_1c = tf.keras.layers.Concatenate(axis=-1)([Inception_1c_3x3,Inception_1c_1x1,Inception_1c_pool_proj,Inception_1c_5x5])\n",
    "    \n",
    "    #       Adición Layer\n",
    "\n",
    "    Addition_1 = tf.keras.layers.Add()([depthcat_1c, Inception_1a_relu])\n",
    "    Addition_1_relu = tf.keras.layers.ReLU()(Addition_1)\n",
    "\n",
    "    # Cuarto bloque\n",
    "    \n",
    "    Inception_1d_3x3_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Addition_1_relu)\n",
    "    Inception_1d_3x3_relu_reduce = tf.keras.layers.ReLU()(Inception_1d_3x3_reduce)\n",
    "    Inception_1d_3x3 = tf.keras.layers.Conv2D(18, (3, 3), padding='same')(Inception_1d_3x3_relu_reduce)\n",
    "    \n",
    "    Inception_1d_1x1 = tf.keras.layers.Conv2D(18, (1, 1))(Addition_1_relu)\n",
    "    \n",
    "    Inception_1d_pool = tf.keras.layers.MaxPooling2D((3, 3), padding='same', strides=(1,1))(Addition_1_relu)\n",
    "    Inception_1d_pool_proj = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1d_pool)\n",
    "\n",
    "    \n",
    "    Inception_1d_5x5_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Addition_1_relu)\n",
    "    Inception_1d_5x5_relu_reduce = tf.keras.layers.ReLU()(Inception_1d_5x5_reduce)\n",
    "    Inception_1d_5x5 = tf.keras.layers.Conv2D(18, (5, 5), padding='same')(Inception_1d_5x5_relu_reduce)\n",
    "\n",
    "    depthcat_1d = tf.keras.layers.Concatenate(axis=-1)([Inception_1d_3x3,Inception_1d_1x1,Inception_1d_pool_proj,Inception_1d_5x5])\n",
    "    Inception_1d_relu = tf.keras.layers.ReLU()(depthcat_1d)\n",
    "\n",
    "    # Quinto bloque\n",
    "    \n",
    "    Inception_1e_3x3_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Inception_1d_relu)\n",
    "    Inception_1e_3x3_relu_reduce = tf.keras.layers.ReLU()(Inception_1e_3x3_reduce)\n",
    "    Inception_1e_3x3 = tf.keras.layers.Conv2D(18, (3, 3), padding='same')(Inception_1e_3x3_relu_reduce)\n",
    "    \n",
    "    Inception_1e_1x1 = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1d_relu)\n",
    "    \n",
    "    Inception_1e_pool = tf.keras.layers.MaxPooling2D((3, 3), padding='same', strides=(1,1))(Inception_1d_relu)\n",
    "    Inception_1e_pool_proj = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1e_pool)\n",
    "\n",
    "    \n",
    "    Inception_1e_5x5_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Inception_1d_relu)\n",
    "    Inception_1e_5x5_relu_reduce = tf.keras.layers.ReLU()(Inception_1e_5x5_reduce)\n",
    "    Inception_1e_5x5 = tf.keras.layers.Conv2D(18, (5, 5), padding='same')(Inception_1e_5x5_relu_reduce)\n",
    "\n",
    "    depthcat_1e = tf.keras.layers.Concatenate(axis=-1)([Inception_1e_3x3,Inception_1e_1x1,Inception_1e_pool_proj,Inception_1e_5x5])\n",
    "    \n",
    "    #       Adición Layer\n",
    "\n",
    "    Addition_2 = tf.keras.layers.Add()([depthcat_1e, Addition_1_relu])\n",
    "    Addition_2_relu = tf.keras.layers.ReLU()(Addition_2)\n",
    "\n",
    "    # Sexto bloque\n",
    "    \n",
    "    Inception_1f_3x3_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Addition_2_relu)\n",
    "    Inception_1f_3x3_relu_reduce = tf.keras.layers.ReLU()(Inception_1f_3x3_reduce)\n",
    "    Inception_1f_3x3 = tf.keras.layers.Conv2D(18, (3, 3), padding='same')(Inception_1f_3x3_relu_reduce)\n",
    "    \n",
    "    Inception_1f_1x1 = tf.keras.layers.Conv2D(18, (1, 1))(Addition_2_relu)\n",
    "    \n",
    "    Inception_1f_pool = tf.keras.layers.MaxPooling2D((3, 3), padding='same', strides=(1,1))(Addition_2_relu)\n",
    "    Inception_1f_pool_proj = tf.keras.layers.Conv2D(18, (1, 1))(Inception_1f_pool)\n",
    "\n",
    "    \n",
    "    Inception_1f_5x5_reduce = tf.keras.layers.Conv2D(16, (1, 1))(Addition_2_relu)\n",
    "    Inception_1f_5x5_relu_reduce = tf.keras.layers.ReLU()(Inception_1f_5x5_reduce)\n",
    "    Inception_1f_5x5 = tf.keras.layers.Conv2D(18, (5, 5), padding='same')(Inception_1f_5x5_relu_reduce)\n",
    "\n",
    "    depthcat_1f = tf.keras.layers.Concatenate(axis=-1)([Inception_1f_3x3,Inception_1f_1x1,Inception_1f_pool_proj,Inception_1f_5x5])\n",
    "\n",
    "    # Normalizacion\n",
    "    Batch_normalization = tf.keras.layers.BatchNormalization()(depthcat_1f)\n",
    "\n",
    "    # Aplanado\n",
    "    Flatten = tf.keras.layers.Flatten()(Batch_normalization)\n",
    "\n",
    "    reshape = tf.keras.layers.Reshape((-1, 1))(Flatten)\n",
    "\n",
    "    # LSTM\n",
    "    Lstm =  tf.keras.layers.LSTM(128, return_sequences=True)(reshape)\n",
    "\n",
    "    dense_classification = tf.keras.layers.Dense(num_classes, activation='softmax', name='classoutput')(Lstm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=sequence, outputs=dense_classification)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de datos\n",
    "\n",
    "# Directorio que contiene los archivos JSON\n",
    "directorio = 'StaticDataClean'\n",
    "\n",
    "# Patrón para buscar archivos JSON\n",
    "patron_archivos = os.path.join(directorio, '*.json')\n",
    "\n",
    "# Obtener la lista de archivos JSON en el directorio\n",
    "archivos_json = glob.glob(patron_archivos)\n",
    "\n",
    "# Lista para almacenar los DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterar sobre los archivos JSON\n",
    "for archivo_json in archivos_json:\n",
    "    # Cargar el archivo JSON como DataFrame\n",
    "    df = pd.read_json(archivo_json)\n",
    "    # Agregar el DataFrame a la lista\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combinar los DataFrames en uno solo\n",
    "df_completo = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filtrar los valores nulos en la columna \"Spectograms\"\n",
    "#df_completo_filtrado = df_completo.dropna(subset=['Spectograms'])\n",
    "\n",
    "#df_completo.to_json('static.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Spectograms    Gesture  \\\n",
      "0       [[[0.0002467878, 0.0024613167, 0.0149494895000...  noGesture   \n",
      "1       [[[0.0003451335, 0.0006066799, 0.0084071702, 0...  noGesture   \n",
      "2       [[[0.0006862950000000001, 0.001537076800000000...  noGesture   \n",
      "3       [[[0.0006499612000000001, 0.0009276684, 0.0081...     waveIn   \n",
      "4       [[[0.0005040380000000001, 0.000175695400000000...     waveIn   \n",
      "...                                                   ...        ...   \n",
      "133320  [[[0.0017014866, 0.0049264569, 0.0079292931, 0...  noGesture   \n",
      "133321  [[[2.2472400000000002e-05, 0.00082584020000000...  noGesture   \n",
      "133322  [[[0.0038996293, 0.0040509496, 0.0044628899, 0...  noGesture   \n",
      "133323  [[[0.0042860582, 0.0040204512000000005, 0.0062...  noGesture   \n",
      "133324  [[[0.0018410052, 0.019640409100000002, 0.00990...  noGesture   \n",
      "\n",
      "        Timestamp                                  Quat_Spectrograms  \n",
      "0             240  [[[7.899e-07, 1.5798e-06, 6.5827000000000005e-...  \n",
      "1             255  [[[7.8992e-06, 4.7395e-06, 1.647216506e-17, 2....  \n",
      "2             270  [[[1.8432e-06, 2.8964e-06, 2.3698e-06, 2.63300...  \n",
      "3             285  [[[4.7395e-06, 8.4258e-06, 1.0532000000000001e...  \n",
      "4             300  [[[7.3726e-06, 7.8992e-06, 8.4258e-06, 4.47620...  \n",
      "...           ...                                                ...  \n",
      "133320        750  [[[4.1076000000000004e-05, 9.71605e-05, 1.6851...  \n",
      "133321        765  [[[6.21406e-05, 0.0001390264, 1.05323e-05, 9.5...  \n",
      "133322        780  [[[5.18716e-05, 0.0001356034, 1.23755e-05, 9.1...  \n",
      "133323        795  [[[3.21235e-05, 9.29476e-05, 2.633000000000000...  \n",
      "133324        810  [[[3.10703e-05, 7.58326e-05, 2.185450000000000...  \n",
      "\n",
      "[133325 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Muestra de los datos\n",
    "print(df_completo)\n",
    "df_completo.to_json('static.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide los datos en entrenamiento y validación\n",
    "train_df, val_df = train_test_split(df_completo, test_size=0.2, random_state=42)\n",
    "\n",
    "# Obtén las características y las etiquetas de entrenamiento y validación\n",
    "x_train = train_df['Spectograms'].values\n",
    "y_train = train_df['Gesture'].values\n",
    "x_val = val_df['Spectograms'].values\n",
    "y_val = val_df['Gesture'].values\n",
    "\n",
    "# Convierte las características y las etiquetas en matrices NumPy\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\USER\\Documents\\Tesis\\CodigoTesis\\pruebas.ipynb Cell 5\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/Tesis/CodigoTesis/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39mvalidation_patience)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/Tesis/CodigoTesis/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Entrenamiento del modelo\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/Tesis/CodigoTesis/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49mmax_epochs, batch_size\u001b[39m=\u001b[39;49mmini_batch_size, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/Tesis/CodigoTesis/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(x_val, y_val), shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Documents/Tesis/CodigoTesis/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[lr_scheduler, early_stopping])\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\curso_deep_learning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\curso_deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "# Configuración de las opciones de entrenamiento\n",
    "gpu_device = \"/gpu:1\"\n",
    "max_epochs = 60\n",
    "mini_batch_size = 64\n",
    "initial_learn_rate = 0.04\n",
    "learn_rate_drop_factor = 0.2\n",
    "learn_rate_drop_period = 8\n",
    "gradient_threshold = 1\n",
    "validation_patience = 5\n",
    "\n",
    "input_size = (13,24,8)\n",
    "num_classes = 6\n",
    "\n",
    "# Configuración del entorno GPU\n",
    "with tf.device(gpu_device):\n",
    "    # Definir la arquitectura de la red neuronal\n",
    "    model = set_neural_network_architecture(input_size, num_classes)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer=Adam(learning_rate=initial_learn_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Configuración de las callbacks\n",
    "lr_scheduler = LearningRateScheduler(lambda epoch, lr: lr * learn_rate_drop_factor if epoch % learn_rate_drop_period == 0 else lr)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=validation_patience)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(x_train, y_train, epochs=max_epochs, batch_size=mini_batch_size, \n",
    "                    validation_data=(x_val, y_val), shuffle=False, verbose=1,\n",
    "                    callbacks=[lr_scheduler, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[['loss', 'val_loss']].plot(figsize=(10, 6))\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[['accuracy', 'val_accuracy']].plot(figsize=(10, 6))\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curso_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
